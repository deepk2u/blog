---
title: "Blog: Running Kubeflow in an Enterprise - enmeshed in the service mesh"
description: "Installing Kubeflow 1.2 in existing k8s cluster, Istio service mesh and Argo"
layout: post
toc: true
comments: true
image: images/logo.png
hide: false
categories: [kubeflow, kubeflow 1.2, install kubeflow, kubeflow pipelines, intuit, istio, service mesh, argo]
permalink: /kubeflow/
author: "<a href='https://www.linkedin.com/in/deepk2u/'>Deepak Kumar</a>"
---

Deploying Kubeflow 1.2 in an enterprise with existing k8s infrastructure, a Service Mesh (Istio) and Argo has its own host of challenges.
This blog will address how those challenges were overcome while retaining the best practices that both, the organization and Kubeflow prescribe.


## Lay of the land at Intuit

Intuit has invested heavily in building out a robust Kubernetes infrastructure that powers all of Intuit's products - TurboTax, QuickBooks and Mint. There are 1000s of services that run on 100+ K8s clusters. Managing these clusters is the Intuit Kubernetes Service (IKS) control plane. The IKS control plane provides services such as namespace management, role management and isolation etc. Connecting the services is an advanced Istio based service mesh, which complements Intuit's API Gateway and in combination, they provide robust authentication, authorization, rate limiting and other routing capabilities.

The Intuit ML Platform is built on this ecosystem, and provides model training, inference and feature management capabilities - leveraging the best of Intuit's K8s infrastructure and AWS SageMaker. This is the backdrop against which we started exploring Kubeflow to provide advanced orchestration, experimentation and other services.

## Kubeflow and Istio

Our first challenge with running Kubeflow was the compatibility of Kubeflow's Istio with Intuit's existing Service Mesh built on top of Istio. Two key problems emerge: version compatibility, and operational maintenance.

Kubeflow v1.2 defaults to an older version of Istio (v1.3), which is incompatible with the newer versions of Istio (v1.6), which is what Intuit runs on. Running two Istio versions is impractical, as that would defeat the benefit of a large interconnected existing service mesh. Hence, we wanted for Kubeflow to work seamlessly with Intuit's service mesh, running the newer version of Istio (v1.6)

If you are new to Istio, you might want a primer on these key [Traffic Management Components](https://istio.io/latest/docs/reference/config/networking/):

1. VirtualService
2. DestinationRule
3. Gateway
4. EnvoyFilter

### Step 1: Remove default Istio configurations from Kubeflow

The first step to running Kubeflow was to remove the Istio bundled with Kubeflow, so that it can be integrated with the Intuit service mesh.

Little bit of background about Istio's backward incompatibility between v1.6 and v1.3:

- If you compare the Istio architecture, the core control plane has changed a lot
  - Control plane has 3 different components. Pilot, Galley, Citadel
  - While in [older version](https://istio.io/v1.3/docs/concepts/what-is-istio/#architecture) these components are independent and directly talk to different outside components
  - But in [newer version](https://istio.io/v1.6/docs/ops/deployment/architecture/) all the components sit behind another core component Istiod

  <img src="/images/2021-03-20-kubeflow-1.2-install/istio-1-3-arch.svg" width="" alt="Istio 1.3 Architecture" title="Istio 1.3 Acrhitecture">

  <img src="/images/2021-03-20-kubeflow-1.2-install/istio-1-6-arch.svg" width="" alt="Istio 1.6 Architecture" title="Istio 1.6 Acrhitecture">

  - By default Kubeflow assumes that components like Pilot will be available for configuration data and some of the deployments like `cluster-local-gateway` assumes Pilot is available, while when using Istio v1.6, that is not true
  ```yaml
   --discoveryAddress
   istio-pilot.istio-system:15010
   ```
  - As a result the container never comes up
- Some of the Istio k8s manifests like EnvoyFilter have changed drastically and not compatible at all with new Istio

**To Remove Kubeflow default Istio:**

1. Goto the [kfctl-k8s-istio-dex manifest](https://www.kubeflow.org/docs/started/k8s/kfctl-istio-dex/)
2. Remove Istio from the manifest by following the [Instructions here](https://www.kubeflow.org/docs/started/k8s/kfctl-istio-dex/#notes-on-the-configuration-file)
3. Re-build the manifests using kfctl build command following [these instructions](https://www.kubeflow.org/docs/started/k8s/kfctl-k8s-istio/#alternatively-set-up-your-configuration-for-later-deployment)

### Step 2: Kustomize the Kubeflow manifests

Given the managed K8s ecosystem at Intuit, protocols for service to service communication and namespace isolation is opinionated, and we had to make following changes:

1. Enable Kubeflow namespace for [Istio injection](https://istio.io/latest/docs/setup/additional-setup/sidecar-injection/#automatic-sidecar-injection) by adding the label `istio-injection: enabled` in the namespace specification. This label is then used by Istio to add the sidecar into the namespace
2. Enable sidecar injection to all the deployments and statefulsets in kubeflow by adding annotation `sidecar.istio.io/inject: "true"` along with some Intuit specific custom labels and annotations to the Deployments and StatefulSets
3. Intuit's security policies forbid the direct use of external container registries. Intuit's internal container registry runs regular vulnerability scans and certifies docker images for use in various environments. The internal container registry also has an allow list that enables external registries to be proxied and held to the same high security standards. This was enabled for all Kubeflow containers.
4. Changes in VirtualService to route all the traffic from one central gateway instead of using Kubeflow gateway

We have used [Kustomize](https://kustomize.io/) to modify the Kubeflow application manifest.

1. For adding labels, we have used LabelTransformer
    ```yaml
        apiVersion: builtin
        kind: LabelTransformer
        metadata:
          name: deployment-labels
        labels:
          <Intuit custom labels>
          istio-injected: "true"
        fieldSpecs:
        - path: spec/template/metadata/labels
          kind: Deployment
          create: true
        - path: spec/template/metadata/labels
          kind: StatefulSet
          create: true
    ```
2. For adding annotations, we have used AnnotationsTransformer

    ```yaml
    apiVersion: builtin
    kind: AnnotationsTransformer
    metadata:
      name: deployment-annotations
    annotations:
      <Intuit custom annotations>
      sidecar.istio.io/inject: "true"
    fieldSpecs:
    - path: spec/template/metadata/annotations
      kind: Deployment
      create: true
    - path: spec/template/metadata/annotations
      kind: StatefulSet
      create: true
    ```

3. For replacing docker image urls, we used ImageTagTransformer

    ```yaml
    apiVersion: builtin
    kind: ImageTagTransformer
    metadata:
      name: image-transformer-1
    imageTag:
      name: gcr.io/ml-pipeline/cache-deployer
      newName: docker.intuit.com/gcr-rmt/ml-pipeline/cache-deployer
    ```

4. For transforming VirtualServices

    ```yaml
    - op: remove
      path: /spec/hosts/0
    - op: replace
      path: /spec/gateways/0
      value: <custom gateway>
    - op: add
      path: /spec/hosts/0
      value: <kubflow host name>
    - op: add
      path: /spec/exportTo
      value: ["."]
    ```
5. To wire everything up, we used Kustomization manifest

    ```yaml
    apiVersion: kustomize.config.k8s.io/v1beta1
    kind: Kustomization

    resources:
    - kubeflow.yaml

    transformers:
    - transformers/image-transformers.yaml
    - transformers/label-transformers.yaml
    - transformers/annotations-transformers.yaml

    patchesJson6902:
    # patch VirtualService with explicit host
    # add multiple targets like below for all the VirtualServices which you need
    - path: patches/virtual-service-hosts.yaml
      target:
        group: networking.istio.io
        version: v1alpha3
        kind: VirtualService
        name: centraldashboard
    ```
6. You might face issues with `metadata_envoy` service, in our case we were getting following error
    ```
    [debug][init] [external/envoy/source/common/init/watcher_impl.cc:27] init manager Server destroyed
    unable to bind domain socket with id=0 (see --base-id option)
    2021-01-29T23:32:26.680310Z	error	Epoch 0 exited with error: exit status 1
    ```

    After looking up, we found that, when you run this docker image with Istio Sidecar injection, this problem occurs.
    The reason behind that is, both these containers are essentially envoyproxy containers and default base-id for both
    containers is set to 0.
    
    So to make it work, we had to change CMD in this [Dockerfile](https://github.com/kubeflow/pipelines/blob/1.4.1/third_party/metadata_envoy/Dockerfile#L27)

    ```
    CMD ["/etc/envoy.yaml", "--base-id", "1"]
    ``` 

### Step 3: Fixing the authn/authz

After doing all these changes, authentication was still not working. After digging some more, we found that there are 2 major components around authentication

1. Authservice: It is a StatefulSet that runs the oidc-auth service. It runs in istio-system namespace and directly talks to any oidc service for authentication
2. Authn-filter: It's an EnvoyFilter which filters the traffic to authservice and checks the Kubeflow auth header and redirects to authservice if request is not authorized, check the presence of header called `kubeflow-userid`

Note: Intuit SSO supports OIDC so we did not need to use dex for the integration, but if your org SSO does not support OIDC, then you can use dex in the middle, details can be found [here](https://www.kubeflow.org/docs/started/k8s/kfctl-istio-dex/).

For our installation, we needed the authservice to be mesh enabled and it made more sense to move authservice to `kubeflow` namespace as well, which was already enabled for istio sidecar injection.

Even after enabling mesh on `authservice`, auth was not working, after investigation we found 2 problems:

1. `authservice` Pod was not able to talk to Intuit SSO https url.
As authservice is mesh enabled, outbound traffic from the main container pod is intercepted by istio sidecar to enforce mtls (default behavior). So we had to exclude the https port (443) to disable mtls. This can be done using the annotation `traffic.sidecar.istio.io/excludeOutboundPorts: "443"`

2. `authn-filter`, which is essentially an EnvoyFilter, Istio's specification has changed drastically and old specifications are not backward compatible with newer ones.

  As we are running a newer version of Istio, we changed the EnvoyFilter manifest to retrofit with a newer specification.

  ```yaml
  apiVersion: networking.istio.io/v1alpha3
  kind: EnvoyFilter
  metadata:
    labels:
      app.kubernetes.io/component: oidc-authservice
      app.kubernetes.io/name: oidc-authservice
    name: authn-filter
    namespace: istio-system
  spec:
    configPatches:
    - applyTo: HTTP_FILTER
      match:
        context: GATEWAY
        listener:
          filterChain:
            filter:
              name: envoy.http_connection_manager
      patch:
        operation: INSERT_BEFORE
        value:
          config:
            httpService:
              authorizationRequest:
                allowedHeaders:
                  patterns:
                  - exact: cookie
                  - exact: X-Auth-Token
              authorizationResponse:
                allowedUpstreamHeaders:
                  patterns:
                  - exact: kubeflow-userid
              serverUri:
                cluster: outbound|8080||authservice.kubeflow.svc.cluster.local
                failureModeAllow: false
                timeout: 10s
                uri: http://authservice.kubeflow.svc.cluster.local
            statusOnError:
              code: GatewayTimeout
          name: envoy.ext_authz
  ```

**Deep diving into EnvoyFilter above:**

A side-by-side comparison for you to visualize the changes we had to make:

<img src="/images/2021-03-20-kubeflow-1.2-install/envoy-filter.png" width="" alt="EnvoyFilter" title="">

Note: In our existing enterprise Istio installation, we have couple of EnvoyFilters already in place, and we wanted our filter to be applied before some of the filters, that's why we have filter chain operation INSERT_BEFORE, which is essentially in `envoy.http_connection_manager` filter chain. If you have a similar setup, keep that in mind about when to apply the filter

### Step 4: Setting up Ingress

We exposed the `istio-ingressgateway` service as LoadBalancer using the following mechanism.

1. Setting up public hosted zone in Route 53, add host name you would like to use, like `example.com`
2. Setup ACM certificate for the host name you want to use for the Kubeflow installation, host name can be `kubeflow.example.com`
3. Update the service manifest by adding few annotations
```yaml
# Note that the backend talks over HTTP.
service.beta.kubernetes.io/aws-load-balancer-backend-protocol: http
# TODO: Fill in with the ARN of your certificate.
service.beta.kubernetes.io/aws-load-balancer-ssl-cert: <cert arn from step 2>
service.beta.kubernetes.io/aws-load-balancer-security-groups: <to restrict access within org>
# Only run SSL on the port named "https" below.
service.beta.kubernetes.io/aws-load-balancer-ssl-ports: "https"
external-dns.alpha.kubernetes.io/hostname: kubeflow.example.com
```

After applying the new manifest, aws will automatically add the appropriate A and TXT entries in your hosted zone (`example.com`) and kubeflow will be accessible at `kubeflow.example.com`

To secure the [Gateway with https](https://www.kubeflow.org/docs/started/k8s/kfctl-istio-dex/#secure-with-https) you can also change the gateway port and add key and certificate in the Gateway.

More about these annotations can be found at [Terminate HTTPS traffic on Amazon EKS](https://aws.amazon.com/premiumsupport/knowledge-center/terminate-https-traffic-eks-acm/) and [SSL support on AWS](https://kubernetes.io/docs/concepts/services-networking/service/#ssl-support-on-aws) blog.

### Step 5: Using external Argo installation

Kubfelow uses argo workflows internally to run the pipeline in a workflow fashion. Argo generates artifacts after the workflow steps and all we need to do is configure the artifact store if we are planning to use the external argo.

1. [Install argo workflows](https://github.com/argoproj/argo-workflows/blob/master/docs/quick-start.md#install-argo-workflows) in your cluster, it gets installed in a namespace called argo
2. Remove all the argo related manifests from Kubeflow
3. To override the artifact store, you would need to change the ConfigMap workflow-controller-configmapwhich comes with the [Kubeflow manifest](https://github.com/kubeflow/manifests/blob/306d02979124bc29e48152272ddd60a59be9306c/argo/base/config-map.yaml). It uses minio as the store but you can configure it to use s3 as well. More details can be found from the [Argo](https://github.com/argoproj/argo-workflows/blob/master/docs/workflow-controller-configmap.md)[Workflow Controller Configmap github page](https://github.com/argoproj/argo-workflows/blob/master/docs/workflow-controller-configmap.md)
4. With latest version of Argo there is option to override [artifact store for namespace](https://argoproj.github.io/argo-workflows/artifact-repository-ref/) as well

**Debugging tricks:**

1. Check if EnvoyFilter is getting applied: You should have **istioctl** cmd tool, makes the job really easier, easier to install using brew

    `istioctl proxy-config listeners \&lt;pod name\&gt; --port 15001 -o json`

    See if the envoy filter is getting listed in the output. More about istio proxy debugging can be found [here](https://istio.io/latest/docs/ops/diagnostic-tools/proxy-cmd/).

2. Checking istio-ingressgateway

    ```
    # Port forward to the first istio-ingressgateway pod
    kubectl -n istio-system port-forward $(kubectl -n istio-system get pods -listio=ingressgateway -o=jsonpath="{.items[0].metadata.name}") 15000

    # Get the http routes from the port-forwarded ingressgateway pod (requires jq)
    curl --silent http://localhost:15000/config_dump | jq '\''.configs.routes.dynamic_route_configs[].route_config.virtual_hosts[]| {name: .name, domains: .domains, route: .routes[].match.prefix}'\''

    # Get the logs of the first istio-ingressgateway pod
    # Shows what happens with incoming requests and possible errors
    kubectl -n istio-system logs $(kubectl -n istio-system get pods -listio=ingressgateway -o=jsonpath="{.items[0].metadata.name}") --tail=300

    # Get the logs of the first istio-pilot pod
    # Shows issues with configurations or connecting to the Envoy proxies
    kubectl -n istio-system logs $(kubectl -n istio-system get pods -listio=pilot -o=jsonpath="{.items[0].metadata.name}") discovery --tail=300
    ```

3. Checking the authservice connectivity: istio-ingressgatewaypod should be able to access authservice. You can check that using command:

    `kubectl -n istio-system exec $(kubectl -n istio-system get pods -listio=pilot -o=jsonpath="{.items[0].metadata.name}") -- curl -v http://authservice.istio-system.svc.cluster.local:8080`

    Also make sure authservice is able to reach dex:

    In our case, authservice is in the kubeflow namespace so we made changes accordingly using the command below:

    `kubectl -n kubeflow exec authservice-0 -- wget -q -S -O '-' <oidc auth url>/.well-known/openid-configuration`

    It should show something like below:
    ```javascript
    {
      "issuer": "http://dex.kubeflow.svc.cluster.local:5556/dex",
      "authorization_endpoint": "http://dex.kubeflow.svc.cluster.local:5556/dex/auth",
      "token_endpoint": "http://dex.kubeflow.svc.cluster.local:5556/dex/token",
      "jwks_uri": "http://dex.kubeflow.svc.cluster.local:5556/dex/keys",
      "userinfo_endpoint": "http://dex.kubeflow.svc.cluster.local:5556/dex/userinfo",
      "response_types_supported": [
        "code"
      ],
      "subject_types_supported": [
        "public"
      ],
      "id_token_signing_alg_values_supported": [
        "RS256"
      ],
      "scopes_supported": [
        "openid",
        "email",
        "groups",
        "profile",
        "offline_access"
      ],
      "token_endpoint_auth_methods_supported": [
        "client_secret_basic"
      ],
      "claims_supported": [
        "aud",
        "email",
        "email_verified",
        "exp",
        "iat",
        "iss",
        "locale",
        "name",
        "sub"
      ]
    }
    ```
  
4. Checking connectivity between services: try using **curl** or **wget** from one service to another, usually one or the other is always available otherwise you can always install using apt-get command. Example use case: from ml-pipeline deployment pod you can check if pipeline apis are accessible?
  
    `kubectl -n kubeflow exec $(kubectl -n kubeflow get pods -lapp=ml-pipeline-ui -o=jsonpath="{.items[0].metadata.name}")  -- wget -q -S -O '-' ml-pipeline.kubeflow.svc.cluster.local:8888/apis/v1beta1/pipelines`

## Asks for the Kubeflow Community

The challenges that we encountered at Intuit are not unique, and will be faced by any enterprise that wants to adopt Kubeflow.

It would be nice to have Kubeflow play well with the available K8s infrastructure in an enterprise, rather than mandating its own set of infrastructure. Here are some suggestions/bugs for improvement to the ecosystem, some of which Intuit will work with the community to build out:

1. Kfctl does not seem to be enough for manifest management in enterprise.We see Kubeflow manifest repo is going through [major folder restructuring](https://github.com/kubeflow/manifests/issues/1735) for v1.3 but we think there is still room for improvements.
2. Multi Cluster / Multi Region support. [#5467](https://github.com/kubeflow/kubeflow/issues/5467)
3. Upgrade seems to be an issue in general, should figure out a way to manage this better. [#5440](https://github.com/kubeflow/kubeflow/issues/5440)
4. Multi-tenancy with group support. [#4188](https://github.com/kubeflow/kubeflow/issues/4188)
5. Installing Kubeflow in any custom namespace. [#5647](https://github.com/kubeflow/kubeflow/issues/5647)
6. Existing metadata service is not performant, we did try some settings with more resources and horizontal scaling. Community is already working on [KFP v2.0](https://docs.google.com/document/d/1fHU29oScMEKPttDA1Th1ibImAKsFVVt2Ynr4ZME05i0/edit), which might address alot of concerns around metadata service.

**References**

- [Multi-user, auth-enabled Kubeflow with kfctl_istio_dex](https://www.kubeflow.org/docs/started/k8s/kfctl-istio-dex/)
- [Kubeflow Pipelines (KFP) v2 System Design](https://docs.google.com/document/d/1fHU29oScMEKPttDA1Th1ibImAKsFVVt2Ynr4ZME05i0/edit)
- [Traffic Management Components](https://istio.io/latest/docs/reference/config/networking/)
- [Istio 1.6 Architecture](https://istio.io/v1.6/docs/ops/deployment/architecture/)
- [Istio 1.3 Architecture](https://istio.io/v1.3/docs/concepts/what-is-istio/#architecture)
- [Terminate HTTPS traffic on Amazon EKS](https://aws.amazon.com/premiumsupport/knowledge-center/terminate-https-traffic-eks-acm/)
- [SSL support on AWS](https://kubernetes.io/docs/concepts/services-networking/service/#ssl-support-on-aws)
- [Intuit's Modern SaaS Platform](https://www.developermarch.com/developersummit/downloadPDF/Intuit%20Modern%20SaaS%20Platform%20-%20GIDS.pdf)
- [Stitching a Service Mesh Across Hundreds of Discrete Networks](https://www.youtube.com/watch?v=EWyNbBn1vns)
- [Multicluster Istio configuration and service discovery using Admiral](https://istio.io/latest/blog/2020/multi-cluster-mesh-automation/)
- [Genius of Admiral](https://medium.com/intuit-engineering/genius-of-admiral-3307e63e3ab6)